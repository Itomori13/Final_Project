{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19713e81-d219-4db8-bf7a-bc3337bb6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "def load_data(file_path, sequence_length=24*7, forecast_horizon=24):\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = df['Radiation'].values.reshape(-1, 1)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    data_normalized = scaler.fit_transform(data)\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(data_normalized) - sequence_length - forecast_horizon):\n",
    "        X.append(data_normalized[i:i+sequence_length])\n",
    "        y.append(data_normalized[i+sequence_length:i+sequence_length+forecast_horizon])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    " #data Transformer\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "# the optimized model building\n",
    "def build_optimized_model(input_shape, forecast_horizon):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Simplified CNN feature extraction (L2 regularization added)\n",
    "    x = Conv1D(32, 3, activation='relu', padding='same', \n",
    "              kernel_regularizer=l2(1e-4))(inputs)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Simplified LSTM layer (using single-layer unidirectional LSTM)\n",
    "    lstm_out = LSTM(64, return_sequences=True, \n",
    "                   kernel_regularizer=l2(1e-4))(x)\n",
    "    lstm_out = Dropout(0.3)(lstm_out)\n",
    "    \n",
    "    # Lightweight Attention mechanism (replace original Transformer)\n",
    "    attention = MultiHeadAttention(num_heads=4, key_dim=64)(lstm_out, lstm_out)\n",
    "    attended = LayerNormalization(epsilon=1e-6)(lstm_out + attention)\n",
    "    \n",
    "    # Use hollow convolution strengthen temporal feature extraction\n",
    "    dilated_conv = Conv1D(64, 3, padding='same', dilation_rate=2,\n",
    "                         activation='relu')(attended)\n",
    "    dilated_conv = Dropout(0.3)(dilated_conv)\n",
    "    \n",
    "    # Feature fusion (use addition instead of concatenation to reduce parameters)\n",
    "    merged = attended + dilated_conv\n",
    "    \n",
    "    # feature selection(adaptive)\n",
    "    gap = GlobalAveragePooling1D()(merged)\n",
    "    gmp = GlobalMaxPooling1D()(merged)\n",
    "    combined = concatenate([gap, gmp])\n",
    "    \n",
    "    # output layer\n",
    "    outputs = Dense(forecast_horizon, kernel_regularizer=l2(1e-4))(combined)\n",
    "    outputs = Reshape((forecast_horizon, 1))(outputs)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "class MetricTracker(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = {\n",
    "            'evs': [],\n",
    "            'val_evs': [],\n",
    "            'val_mae': []\n",
    "        }\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Training set metrics\n",
    "        train_pred = self.model.predict(X_train, verbose=0)\n",
    "        y_true = y_train.reshape(-1, 1)\n",
    "        y_pred = train_pred.reshape(-1, 1)\n",
    "        evs = explained_variance_score(y_true, y_pred)\n",
    "        self.metrics['evs'].append(evs)\n",
    "        \n",
    "        # Validation set metrics\n",
    "        val_pred = self.model.predict(X_test, verbose=0)\n",
    "        val_true = y_test.reshape(-1, 1)\n",
    "        val_pred_flat = val_pred.reshape(-1, 1)\n",
    "        val_evs = explained_variance_score(val_true, val_pred_flat)\n",
    "        self.metrics['val_evs'].append(val_evs)\n",
    "        self.metrics['val_mae'].append(logs['val_mae'])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} - EVS: {evs:.4f} | Val EVS: {val_evs:.4f}\")\n",
    "\n",
    "# Use the correct learning rate scheduler initialization\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=initial_learning_rate),  # use initilized learning rate\n",
    "             loss='mse',\n",
    "             metrics=['mae'])\n",
    "# Add learning rate update callback\n",
    "class LRScheduler(Callback):\n",
    "    def __init__(self, schedule):\n",
    "        super().__init__()\n",
    "        self.schedule = schedule\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not self.optimizer:\n",
    "            self.optimizer = self.model.optimizer\n",
    "        new_lr = self.schedule(epoch * BATCH_SIZE)\n",
    "        tf.keras.backend.set_value(self.optimizer.learning_rate, new_lr)\n",
    "\n",
    "# add callback in LRScheduler list\n",
    "callbacks=[\n",
    "    early_stopping,\n",
    "    reduce_lr,\n",
    "    SimplifiedTracker(),\n",
    "    LRScheduler(lr_schedule)  # Added learning rate scheduling callback\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
